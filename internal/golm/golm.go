package golm

import (
	"bytes"
	"encoding/json"
	"net/http"
	"net/url"
	"time"
)

type Client interface {
	ListModels() (ListModelResponse, error)
	Completion(request CompletionRequest) ([]CompletionResponse, error)
}

func NewClient(baseURL string, timeout int) Client {
	return client{
		baseURL: baseURL,
		timeout: timeout,
	}
}

type client struct {
	baseURL string
	timeout int // seconds
}

type ChatCompletionRequest struct {

	// Messages is list of messages comprising the conversation so far. Depending on the model you use,
	// different message types (modalities) are supported, like text, images, and audio.
	// This is a Required field
	Messages []any `json:"messages"`

	// Model is the ID of the model to use. See the model endpoint compatibility table for details on
	// which models work with the Chat API.
	// This is a required field
	Model string `json:"model"`

	// Store refers to whether or not to store the output of this chat completion request for
	// use in our model distillation or evals products.
	// This is an optional field. Default to false
	Store bool `json:"store"`

	// ReasoningEffort refer to o1 and o3-mini models only.
	// This is an optional field. It defaults to medium
	// Constrains effort on reasoning for reasoning models. Currently supported values
	// are low, medium, and high. Reducing reasoning effort can result in faster responses
	// and fewer tokens used on reasoning in a response.
	// This is an optional field
	ReasoningEffort *string `json:"reasoning_effort"`

	// Metadata is a Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional
	// information about the object in a structured format, and querying for objects via API or the dashboard.
	// Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
	// This is an optional field
	Metadata map[string]any `json:"metadata"`

	// FrequencyPenalty is a number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
	// frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	// This is an optional field
	// The default value is 0
	FrequencyPenalty int `json:"frequency_penalty"`

	// LogitBias modify the likelihood of specified tokens appearing in the completion.
	// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
	// to an associated bias value from -100 to 100. Mathematically, the bias is added to the
	// logits generated by the model prior to sampling. The exact effect will vary per model,
	// but values between -1 and 1 should decrease or increase likelihood of selection; values
	// like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	// This is an optional field
	LogitBias string `json:"logit_bias"`

	// LogProbs Whether to return log probabilities of the output tokens or not. If true,
	// returns the log probabilities of each output token returned in the content of message.
	// This is an optional field
	LogProbs *bool `json:"logprobs"`

	// TopLogProbs is an integer between 0 and 20 specifying the number of most likely
	// tokens to return at each token position, each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	// This is an optional field
	TopLogProbs *int `json:"top_logprobs"`

	// MaxTokens the maximum number of tokens that can be generated in the chat completion.
	// This value can be used to control costs for text generated via API.
	// This value is now deprecated in favor of max_completion_tokens, and is not compatible
	// with o1 series models.
	// This is an optional and Deprecated field
	MaxTokens *int `json:"max_tokens"`

	// MaxCompletionTokens is an upper bound for the number of tokens that can be generated for
	// a completion, including visible output tokens and reasoning tokens.
	// This is an optional field.
	MaxCompletionTokens *int `json:"max_completion_tokens"`

	// N is how many chat completion choices to generate for each input message. Note that you
	// will be charged based on the number of generated tokens across all of the choices. Keep
	// n as 1 to minimize costs.
	// This is an optional field
	N *int `json:"n"`

	// Modalities refers to Output types that you would like the model to generate for this request.
	// Most models are capable of generating text, which is the default:
	//
	// ["text"]
	//
	// The gpt-4o-audio-preview model can also be used to generate audio. To request
	// that this model generate both text and audio responses, you can use:
	//
	// ["text", "audio"]
	//
	// This is an optional field
	Modalities []string `json:"modalities"`

	// Prediction is configuration for a Predicted Output, which can greatly
	// improve response times when large parts of the model response are known
	// ahead of time. This is most common when you are regenerating a file with
	// only minor changes to most of the content.
	// This is an optional field.
	Prediction any `json:"prediction"`

	// Audio is parameters for audio output. Required when audio output is
	// requested with modalities: ["audio"].
	// This is an optional field
	Audio *string `json:"audio"`

	// PresencePenalty is number between -2.0 and 2.0. Positive values penalize
	// new tokens based on whether they appear in the text so far, increasing
	// the model's likelihood to talk about new topics.
	// This is an optional field where default value is 0
	PresencePenalty *int `json:"presence_penalty"`

	// ResponseFormat is an object specifying the format that the model must output.
	// Setting to { "type": "json_schema", "json_schema": {...} } enables Structured
	// Outputs which ensures the model will match your supplied JSON schema.
	// Learn more in the Structured Outputs guide.
	// Setting to { "type": "json_object" } enables JSON mode, which ensures the
	// message the model generates is valid JSON.
	// Important: when using JSON mode, you must also instruct the model to produce
	// JSON yourself via a system or user message. Without this, the model may generate an
	// unending stream of whitespace until the generation reaches the token limit, resulting
	// in a long-running and seemingly "stuck" request. Also note that the message content may
	// be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens
	// or the conversation exceeded the max context length.
	// This is an optional field
	ResponseFormat any `json:"response_format"`

	// Seed is feature is in Beta. If specified, our system will make a best effort to
	// sample deterministically, such that repeated requests with the same seed and parameters
	// should return the same result. Determinism is not guaranteed, and you should refer to the
	// system_fingerprint response parameter to monitor changes in the backend.
	// This is an optional field.
	Seed *int `json:"seed"`

	// ServiceTier Specifies the latency tier to use for processing the request. This parameter
	// is relevant for customers subscribed to the scale tier service:
	// If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale
	// tier credits until they are exhausted.
	// If set to 'auto', and the Project is not Scale tier enabled, the request will be processed
	// using the default service tier with a lower uptime SLA and no latency guarantee.
	// If set to 'default', the request will be processed using the default service tier with a
	// lower uptime SLA and no latency guarantee.
	// When not set, the default behavior is 'auto'.
	// This is an optional field with default value set to auto.
	ServiceTier *string `json:"service_tier"`

	// Stop up to 4 sequences where the API will stop generating further tokens.
	// This is an optional field
	Stop []string `json:"stop"`

	// Stream If set, partial message deltas will be sent, like in ChatGPT.
	// Tokens will be sent as data-only server-sent events as they become available,
	// with the stream terminated by a data: [DONE] message.
	// This is an optional field,
	Stream *bool `json:"stream"`

	// StreamOptions options for streaming response. Only set
	// this when you set stream: true.
	// This is an optional field.
	StreamOptions any `json:"stream_options"`

	// Temperature sampling temperature to use, between 0 and 2.
	// Higher values like 0.8 will make the output more random, while
	// lower values like 0.2 will make it more focused and deterministic.
	// We generally recommend altering this or top_p but not both.
	// This is an optional field the default value is 1
	Temperature *int `json:"temperature"`

	// TopP is alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// We generally recommend altering this or temperature but not both.
	// This is an optiona filed with default set to 1
	TopP *int `json:"top_p"`

	// Tools is a list of tools the model may call. Currently, only functions are supported as
	// a tool. Use this to provide a list of functions the model may generate JSON inputs for.
	// A max of 128 functions are supported.
	// This is an optional field.
	Tools any `json:"tools"`

	// ToolChoice controls which (if any) tool is called by the model. none means the model will
	// not call any tool and instead generates a message. auto means the model can pick between
	// generating a message or calling one or more tools. required means the model must call one
	// or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}}
	// forces the model to call that tool.
	// This is an optional field. none is the default when no tools are present. auto is the default
	// if tools are present.
	ToolChoice *string `json:"tool_choice"`

	// ParallelToolCalls whether to enable parallel function calling during tool use.
	// This is an optional feld with value default to true.
	ParallelToolCalls *bool `json:"parallel_tool_calls"`

	// User is a unique identifier representing your end-user, which can help OpenAI
	// to monitor and detect abuse. Learn more.
	// This is an optional field.
	User *string `json:"user"`

	// FunctionCall is DEPRECATED
	// Controls which (if any) function is called by the model.
	// none means the model will not call a function and instead generates a message.
	// auto means the model can pick between generating a message or calling a function.
	// Specifying a particular function via {"name": "my_function"} forces the model
	// to call that function.
	// This is an optional field. none is the default when no functions are present.
	// auto is the default if functions are present.
	FunctionCall any `json:"function_call"`

	// Functions (DEPRECATED)
	// A list of functions the model may generate JSON inputs for.
	// This is an optional field
	Functions any `json:"functions"`
}

func (c client) ChatCompletion(request CompletionRequest) (CompletionResponse, error) {
	endPoint, err := url.JoinPath(c.baseURL, "/v1/completions")
	if err != nil {
		return CompletionResponse{}, err
	}

	reqBody, err := json.Marshal(request)
	if err != nil {
		return CompletionResponse{}, err
	}

	req, err := http.NewRequest(http.MethodPost, endPoint, bytes.NewReader(reqBody))
	if err != nil {
		return CompletionResponse{}, err
	}

	req.Header.Set("Content-Type", "application/json")

	httpClient := http.Client{
		Timeout: time.Duration(c.timeout) * time.Second,
	}
	response, err := httpClient.Do(req)
	if err != nil {
		return CompletionResponse{}, err
	}
	defer response.Body.Close()

	decoder := json.NewDecoder(response.Body)

	var completionResponse CompletionResponse
	if err := decoder.Decode(&completionResponse); err != nil {
		return CompletionResponse{}, err
	}

	return completionResponse, nil
}
